{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, probabilities):\n",
    "        self.probabilities = probabilities\n",
    "        self.n_arms = len(probabilities)\n",
    "\n",
    "    def pull(self, arm):\n",
    "        return 1 if np.random.rand() < self.probabilities[arm] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, n_arms, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            return np.argmax(self.values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] += (reward - value) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb_values = self.values + np.sqrt((2 * np.log(self.total_counts)) / self.counts)\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.total_counts += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] += (reward - value) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.successes = np.ones(n_arms)\n",
    "        self.failures = np.ones(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        sampled = np.random.beta(self.successes, self.failures)\n",
    "        return np.argmax(sampled)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        if reward == 1:\n",
    "            self.successes[chosen_arm] += 1\n",
    "        else:\n",
    "            self.failures[chosen_arm] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(algo_class, bandit, horizon, **kwargs):\n",
    "    algo = algo_class(bandit.n_arms, **kwargs) if kwargs else algo_class(bandit.n_arms)\n",
    "    regret = []\n",
    "    optimal_reward = max(bandit.probabilities)\n",
    "    total_regret = 0\n",
    "\n",
    "    for t in range(horizon):\n",
    "        arm = algo.select_arm()\n",
    "        reward = bandit.pull(arm)\n",
    "        algo.update(arm, reward)\n",
    "        total_regret += optimal_reward - bandit.probabilities[arm]\n",
    "        regret.append(total_regret)\n",
    "\n",
    "    if hasattr(algo, 'values'):\n",
    "        best_arm = np.argmax(algo.values)\n",
    "    elif hasattr(algo, 'successes'):\n",
    "        best_arm = np.argmax(algo.successes / (algo.successes + algo.failures))\n",
    "    else:\n",
    "        best_arm = None\n",
    "\n",
    "    return regret, total_regret, best_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_algos(probabilities, horizon=5000):\n",
    "    bandit = MultiArmedBandit(probabilities)\n",
    "\n",
    "    eps_regret, eps_total, eps_best = run_algorithm(EpsilonGreedy, bandit, horizon, epsilon=0.1)\n",
    "    bandit = MultiArmedBandit(probabilities)\n",
    "    ucb_regret, ucb_total, ucb_best = run_algorithm(UCB, bandit, horizon)\n",
    "    bandit = MultiArmedBandit(probabilities)\n",
    "    ts_regret, ts_total, ts_best = run_algorithm(ThompsonSampling, bandit, horizon)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(eps_regret, label='Epsilon-Greedy')\n",
    "    plt.plot(ucb_regret, label='UCB')\n",
    "    plt.plot(ts_regret, label='Thompson Sampling')\n",
    "    plt.xlabel('Timesteps')\n",
    "    plt.ylabel('Total Regret')\n",
    "    plt.title('Total Regret vs Timesteps')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nFinal Regret:\")\n",
    "    print(f\"Epsilon-Greedy: {eps_total:.2f}, Best Arm: {eps_best}\")\n",
    "    print(f\"UCB: {ucb_total:.2f}, Best Arm: {ucb_best}\")\n",
    "    print(f\"Thompson Sampling: {ts_total:.2f}, Best Arm: {ts_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your bandit here\n",
    "evaluate_all_algos(probabilities=[0.3, 0.5, 0.8, 0.6], horizon=5000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
